{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN ensemble classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas\n",
    "import timeit #more accurate than time\n",
    "\n",
    "import theano\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential, model_from_json, load_model\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "import os\n",
    "import json\n",
    "\n",
    "from six.moves import cPickle #Faster than pickle\n",
    "\n",
    "import sys\n",
    "sys.path.append('../modules/')\n",
    "from MPPlot import *\n",
    "from Processors import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "def set_keras_backend(backend):\n",
    "    \"\"\"\n",
    "    Changes Keras backend\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    backend : str\n",
    "              Backend wanted (theano or tensorflow)\n",
    "              \n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    \n",
    "    idm = {\"theano\": \"th\", \"tensorflow\": \"tf\"}\n",
    "    if K.backend() != backend:\n",
    "        os.environ['KERAS_BACKEND'] = backend\n",
    "        importlib.reload(K)\n",
    "        assert K.backend() == backend\n",
    "        keras.backend.set_image_dim_ordering(idm[backend])\n",
    "\n",
    "set_keras_backend(\"theano\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loc = \"../data/\"\n",
    "data = pandas.read_csv(loc + \"DS_1_train.csv\")\n",
    "sigData = data.signal == 1.0\n",
    "bkgData = data.signal == 0.0\n",
    "print(\"Samples contains {} signal events and {} background events, {} events total\".format(\n",
    "        len(data[sigData]), len(data[bkgData]), len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data and add new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create development and validation samples\n",
    "Development data is used for training and testing. Validation is used for testing the final classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This defines lists of indices for signal and background events for the development and validation samples. About 20% of each class is reserved for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sig_devIndeces, sig_valIndeces = \\\n",
    "                train_test_split([i for i in data[sigData].index.tolist()],\n",
    "                                 test_size=0.2, random_state=1337)\n",
    "bkg_devIndeces, bkg_valIndeces = \\\n",
    "                train_test_split([i for i in data[bkgData].index.tolist()],\n",
    "                                 test_size=0.2, random_state=1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we split the full dataset into the dev and val sets, and define aliases for the cuts necessary to select signal and background events in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "devData = data.loc[sig_devIndeces].copy()\n",
    "devData = devData.append(data.loc[bkg_devIndeces].copy(), ignore_index = True)\n",
    "valData = data.loc[sig_valIndeces].copy()\n",
    "valData = valData.append(data.loc[bkg_valIndeces].copy(), ignore_index = True)\n",
    "\n",
    "sigDev = (devData.signal == 1.0)\n",
    "bkgDev = (devData.signal == 0.0)\n",
    "sigVal = (valData.signal == 1.0)\n",
    "bkgVal = (valData.signal == 0.0)\n",
    "\n",
    "print(\"{} events for training, {} events for validation\".format(len(devData), len(valData)))\n",
    "print(\"Dev: {} of which are signal and {} are background\".format(len(devData[sigDev]), len(devData[bkgDev])))\n",
    "print(\"Val: {} of which are signal and {} are background\".format(len(valData[sigVal]), len(valData[bkgVal])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are more background events, let's define weights to help balance out the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classWeights = {0.0 : 1.0,\n",
    "                1.0 : len(devData[bkgDev])/len(devData[sigDev])}\n",
    "print(\"Dev Weights are: \", classWeights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Options\n",
    "Define the features used for discrimination and training options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainFeatures = ['X', 'Y', 'Z', 'TX', 'TY', 'chi2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set0 = trainFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training options\n",
    "Here we define the way we'll train the classifier. For simplicity we'll just use the low-level final-state features. We can also choose what pre-processing step to apply to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 6 features ['X', 'Y', 'Z', 'TX', 'TY', 'chi2']\n"
     ]
    }
   ],
   "source": [
    "classTrainFeatures = set0 #The features used\n",
    "classModel = \"model0\" #Will define the layout of the network\n",
    "varSet = \"set0\" #Name of the feature set used, mainly for saving results\n",
    "normIn = True #Whether we want to normalise and standardise the inputs\n",
    "pca = True #Whether we want to use principal-component analysis to decorrelate inputs\n",
    "whiten = False #Whether we want to whiten input data\n",
    "nSplits = 10 #Number of train/test splits to make during cross-validation\n",
    "ensembleSize = 8 #Number of classifiers  to include in ensemble = min(nSplits, ensembleSize)\n",
    "ensembleMode = 'AUC' #Metric used to weight classifiers in ensemble, I've found loss to quite relaible\n",
    "compileArgs = {'loss':'binary_crossentropy', \n",
    "               'optimizer':'nadam'} #Loss function and optimiser for NN\n",
    "trainParams = {'epochs' : 10000, \n",
    "               'batch_size' : 64, \n",
    "               'verbose' : 0} #Maximum epochs for training and size of mini-batch\n",
    "print(\"Training on {} features {}\". format(len(classTrainFeatures),[var for var in classTrainFeatures]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a SK-Learn pipeline which will contain transformation steps for any data fed in. Pipelines are a nice, compact way of handing data transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stepsIn = []\n",
    "if not normIn and not pca:\n",
    "    stepsIn.append(('ident', StandardScaler(with_mean=False, with_std=False))) #For compatability\n",
    "else:\n",
    "    if normIn:\n",
    "        stepsIn.append(('normIn', StandardScaler()))\n",
    "    if pca:\n",
    "        stepsIn.append(('pca', PCA(whiten=whiten)))\n",
    "inputPipe = Pipeline(stepsIn)\n",
    "stepsOut = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we fit the pipeline to the **development** data inputs. For compactness we also transform the development data and create Numpy arrays of the inputs and targets. **N.B.** The type of the inputs will normally be either float32 or float64. float32 is preferred, since speed and memory outweighs precision. Sometimes if the data is naturally in float64, the conversion to float32 can can result in NaNs or infs, so watch out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_class = inputPipe.fit_transform(devData[classTrainFeatures].values.astype('float32')) #Inputs\n",
    "y_class = devData['signal'].values.astype('int') #Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define classifier\n",
    "Here we define the layout of the neural network. The basic class is sequential. The network is them defined by adding dense layers (which contain the neurons and weights) and then the activation function. The activation function can be defined directly in the dense constructor, however adding separately gives a clearer picture of the network, and allows it to be easily replaced with an advanced activation-function.\n",
    "\n",
    "For contempory ML, the rectified linear unit is generally the default activation function. Since we want the outputs to be between 0 (background) and 1 (signal) we'll use the sigmoid function as the final activation function, since it saturates at these values and is symmetric between them.\n",
    "\n",
    "The compile step combines the choice of loss function and optimiser into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getClassifier(model, nIn, compileArgs):\n",
    "    classModel = Sequential()\n",
    "    depth = None\n",
    "    width = None\n",
    "    if model == \"model0\":\n",
    "        depth = 3\n",
    "        width = 100\n",
    "    classModel.add(Dense(width, input_dim=nIn, kernel_initializer='he_normal')) #First layer requires number of inputs\n",
    "    classModel.add(Activation('relu')) #Add ReLU activation function\n",
    "    for i in range(depth): #Continue to add hidden layers\n",
    "        classModel.add(Dense(width, kernel_initializer='he_normal')) #Subsequent layers inherit input_dim from previous layer\n",
    "        classModel.add(Activation('relu'))\n",
    "    classModel.add(Dense(1, activation='sigmoid', kernel_initializer='glorot_normal')) #Final layer requires one output\n",
    "    classModel.compile(**compileArgs) #Compile the network graph to prepare it for use\n",
    "    return classModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callbacks are methods that can be called during training. They have a variety of uses such as monitoring training, stopping training early, and saving different versions of the model. Here we define our own callback,  which saves the history of the training.\n",
    "\n",
    "We want to view the history of the model's performance on the training and testing data during data, however by default the training loss is averaged over the epoch, and the test loss is evaluated at the end of the epoch, so is not comparable. This modified version evaluates the performance on the training data at the end of each epoch.\n",
    "\n",
    "Later well also use some other callbacks:\n",
    "\n",
    "EarlyStopping monitors a specified metric and stops the training if the performance fails to improve for a specified number of epochs in a row. Here we use it to monitor the loss on the test data and stop when it doesn't improve after 10 epochs.\n",
    "\n",
    "ModelCheckpoint is used to save the weights of the network during training. It's quite flexible, but here we use it save the model which performs best according to the loss in test data.\n",
    "\n",
    "Normally during training, the test loss will reach a minimum and either saturate or start to increase (overtraining). The training loss will normally either saturate or continue to decrease. The optimum point is when the test-loss first reaches its minimum point. The combination of EarlyStopping and ModelCheckpoint acts to save the model at this point and allow some leeway in case it starts to decrease again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LossHistory(Callback):\n",
    "    def __init__(self, trData):\n",
    "        self.trainingData = trData\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {}\n",
    "        self.losses['loss'] = []\n",
    "        self.losses['val_loss'] = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.losses['loss'].append(self.model.evaluate(self.trainingData[0], self.trainingData[1], verbose=0))\n",
    "        self.losses['val_loss'].append(logs.get('val_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to train the classifier.\n",
    "\n",
    "We use stratified k-fold cross-validation for training. This splits the full development data into *k* sets, trains the model on $k-1$ sets and tests its performance on the remainings set. This then continues *k* times with a different set being used for testing each time. The *stratified* part means that each set will contain the same fraction of event classes as the full dataset, which helps ensure unbiased training and means that our class weights will be valid.\n",
    "\n",
    "During training we save each trained model, as well as its performance on the test set.\n",
    "\n",
    "**N.B.** The model can either be saved directly, or by saving the weights and layout separately. The former is more compact, but doesn't handle custom objects well. If you've used a custom loss or activation function, the the second method is more flexible. It also seems to be quicker, but I've not done concrete tests..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "results = []\n",
    "histories = []\n",
    "\n",
    "os.system(\"mkdir train_weights\")\n",
    "os.system(\"rm train_weights/*.h5\")\n",
    "os.system(\"rm train_weights/*.json\")\n",
    "os.system(\"rm train_weights/*.pkl\")\n",
    "\n",
    "skf = StratifiedKFold(n_splits=nSplits, shuffle=True)\n",
    "\n",
    "i = 0\n",
    "for train, test in skf.split(X_class, y_class): #test and train are sets of indices for the current CV fold\n",
    "    i += 1\n",
    "    print(\"Running fold\", i, \"/\", nSplits)\n",
    "    \n",
    "    foldStart = timeit.default_timer()\n",
    "    \n",
    "    model = None # Clearing the NN\n",
    "    model = getClassifier(classModel, len(classTrainFeatures), compileArgs)\n",
    "    model.reset_states #Just checking\n",
    "    \n",
    "    lossHistory = LossHistory((X_class[train], y_class[train]))\n",
    "    earlyStop = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='auto')\n",
    "    saveBest = ModelCheckpoint(\"train_weights/best.h5\", monitor='val_loss', verbose=0, \n",
    "                               save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "    \n",
    "    #Begin training the model\n",
    "    model.fit(X_class[train], y_class[train], validation_data = (X_class[test], y_class[test]),\n",
    "              class_weight = classWeights, callbacks = [earlyStop, saveBest, lossHistory], **trainParams)\n",
    "    \n",
    "    histories.append(lossHistory.losses) #Saves the loss history from callback\n",
    "    model.load_weights(\"train_weights/best.h5\") #Loads the best model saved by ModelCheckpoint\n",
    "    \n",
    "    results.append({})\n",
    "    results[-1]['loss'] = model.evaluate(X_class[test], y_class[test], verbose=0) #Gets loss on test data\n",
    "    results[-1]['AUC'] = 1-roc_auc_score(y_class[test], model.predict(X_class[test], verbose=0)) #Gets ROC AUC for test data\n",
    "    \n",
    "    print(\"Score is:\", results[-1])\n",
    "    print(\"Fold took {:.3f}s \".format(timeit.default_timer() - foldStart))\n",
    "    \n",
    "    model.save('train_weights/train_' + str(i-1) + '.h5') #Save the model\n",
    "\n",
    "with open('train_weights/resultsFile.pkl', 'wb') as fout: #Save results\n",
    "    cPickle.dump(results, fout)\n",
    "\n",
    "print(\"Cross-validation took {:.3f}s \".format(timeit.default_timer() - start))\n",
    "\n",
    "X_class = None\n",
    "y_class = None\n",
    "train = None\n",
    "test = None\n",
    "model.summary() #Prints a summary of the model layout\n",
    "model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot histories\n",
    "Now let's plot the history of the training.\n",
    "\n",
    "We can see that the test loss starts to decrease, reaches a minimum point, then begins to increase. The training loss continues to decrease. The early stopping detects the lack of imporvement in test loss and stops the training. The checkpoint allows us to use the state of the model at the minimum point of training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "for i, history in enumerate(histories):\n",
    "    if i == 0:\n",
    "        plt.plot(history['loss'], color='g', label='Training')\n",
    "        plt.plot(history['val_loss'], color='b', label='Testing')\n",
    "    else:\n",
    "        plt.plot(history['loss'], color='g')\n",
    "        plt.plot(history['val_loss'], color='b')\n",
    "        \n",
    "plt.legend(fontsize=16)\n",
    "plt.xlabel(\"Epoch\", fontsize=24, color='black')\n",
    "plt.ylabel(\"Loss\", fontsize=24, color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct ensemble\n",
    "During the *k*-fold CV we trained *k* models. We could just use the best one, however it is unlikely to optimimum for all input possibilities. Ensembling is a method of using multiple classifiers together to achieve a better result than a single one on its own.\n",
    "\n",
    "The method I use here is to weight the contributions of each classifier according to how well it performed on its test set during CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = None\n",
    "with open('train_weights/resultsFile.pkl', 'rb') as fin: #Reload results in case notebook was closed\n",
    "    results = cPickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadModel(cycle, location='train_weights/train_'): #Function to load a specified classifier\n",
    "    cycle = int(cycle)\n",
    "    model = load_model(location + str(cycle) + '.h5')\n",
    "    model.compile(**compileArgs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getWeights(value, met): #How the weight is calculated. Metrics configured such that lower values are better.\n",
    "    return 1/value #Reciprocal of metric is a simple way of assigning larger weight s to better metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we order the classifiers by performance, load the required number, and weight them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ensemble = []\n",
    "weights = []\n",
    "\n",
    "print(\"Choosing ensemble by\", ensembleMode)\n",
    "dtype = [('cycle', int), ('result', float)]\n",
    "values = np.sort(np.array([(i, result[ensembleMode]) for i, result in enumerate(results)], \n",
    "                          dtype=dtype), order=['result'])\n",
    "\n",
    "for i in range(min([ensembleSize, len(results)])):\n",
    "    ensemble.append(loadModel(values[i]['cycle']))\n",
    "    weights.append(getWeights(values[i]['result'], ensembleMode))\n",
    "    print(\"Model {} is {} with {} = {}\". format(i, values[i]['cycle'], ensembleMode, values[i]['result']))\n",
    "    \n",
    "weights = np.array(weights)\n",
    "weights = weights/weights.sum() #normalise weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response on dev data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply the ensemble to the whole of the development data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dev = inputPipe.transform(devData[classTrainFeatures].values.astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict(inData, ensemble, weights, n=-1): #Loop though each classifier and predict data class\n",
    "    pred = np.zeros((len(inData), 1))\n",
    "    if n == -1:\n",
    "        n = len(ensemble)+1\n",
    "    ensemble = ensemble[0:n] #Use only specified number of classifiers\n",
    "    weights = weights[0:n]\n",
    "    weights = weights/weights.sum() #Renormalise weights\n",
    "    \n",
    "    for i, model in enumerate(ensemble):\n",
    "        pred += weights[i] * model.predict(inData, verbose=0)\n",
    "        \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = predict(X_dev, ensemble, weights)\n",
    "devData['pred_class'] = pandas.Series(pred[:,0], index=devData.index) #Add predicted class to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "devAUC = roc_auc_score(devData[\"signal\"].values.astype('int'), devData['pred_class'])\n",
    "print('Area under ROC curve for development data is {:.5f}'.format(devAUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble testing\n",
    "What benefit does ensembling bring? Let's take a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(ensembleSize):\n",
    "    auc = roc_auc_score(devData[\"signal\"].values.astype('int'),\n",
    "                        predict(X_dev, ensemble, weights, i+1))\n",
    "    if not i:\n",
    "        print(\"AUC using best classifier:\\t{:.5f}\".format(auc))\n",
    "    else:\n",
    "        print(\"AUC using {} classifiers:\\t{:.5f}\".format(i+1, auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definite improvements seen, but the improvements will eventually saturate. The number to use and the ways the weights are calculated are hyperparameters of the MVA. We could have set aside some of the development data for tuning this, but for now we'll just stick with what we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response on val data\n",
    "Having done all the development of the classifier, we're now ready to do final testing on the withheld validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_val = inputPipe.transform(valData[classTrainFeatures].values.astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = predict(X_val, ensemble, weights)\n",
    "valData['pred_class'] = pandas.Series(pred[:,0], index=valData.index) #Add predicted class to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valAUC = roc_auc_score(valData[\"signal\"].values.astype('int'), valData['pred_class'])\n",
    "print('Area under ROC curve for validation data is {:.5f}'.format(valAUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble testing\n",
    "Again we can confirm that ensembling helps. **N.B.** Do not use the validation data for any tuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(ensembleSize):\n",
    "    auc = roc_auc_score(valData[\"signal\"].values.astype('int'),\n",
    "                              predict(X_val, ensemble, weights, i+1))\n",
    "    if not i:\n",
    "        print(\"AUC using best classifier:\\t{:.5f}\".format(auc))\n",
    "    else:\n",
    "        print(\"AUC using {} classifiers:\\t{:.5f}\".format(i+1, auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct ensemble\n",
    "During the *k*-fold CV we trained *k* models. We could just use the best one, however it is unlikely to optimimum for all input possibilities. Ensembling is a method of using multiple classifiers together to achieve a better result than a single one on its own.\n",
    "\n",
    "The method I use here is to weight the contributions of each classifier according to how well it performed on its test set during CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensembleSize = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = None\n",
    "with open('train_weights/resultsFile.pkl', 'rb') as fin: #Reload results in case notebook was closed\n",
    "    results = cPickle.load(fin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we order the classifiers by performance, load the required number, and weight them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ensemble = []\n",
    "weights = []\n",
    "\n",
    "print(\"Choosing ensemble by\", ensembleMode)\n",
    "dtype = [('cycle', int), ('result', float)]\n",
    "values = np.sort(np.array([(i, result[ensembleMode]) for i, result in enumerate(results)], \n",
    "                          dtype=dtype), order=['result'])\n",
    "\n",
    "for i in range(min([ensembleSize, len(results)])):\n",
    "    ensemble.append(loadModel(values[i]['cycle']))\n",
    "    weights.append(getWeights(values[i]['result'], ensembleMode))\n",
    "    print(\"Model {} is {} with {} = {}\". format(i, values[i]['cycle'], ensembleMode, values[i]['result']))\n",
    "    \n",
    "weights = np.array(weights)\n",
    "weights = weights/weights.sum() #normalise weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response on dev data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply the ensemble to the whole of the development data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dev = inputPipe.transform(devData[classTrainFeatures].values.astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = predict(X_dev, ensemble, weights)\n",
    "devData['pred_class'] = pandas.Series(pred[:,0], index=devData.index) #Add predicted class to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "devAUC = roc_auc_score(devData[\"signal\"].values.astype('int'), devData['pred_class'])\n",
    "print('Area under ROC curve for development data is {:.5f}'.format(devAUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response on val data\n",
    "Having done all the development of the classifier, we're now ready to do final testing on the withheld validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_val = inputPipe.transform(valData[classTrainFeatures].values.astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = predict(X_val, ensemble, weights)\n",
    "valData['pred_class'] = pandas.Series(pred[:,0], index=valData.index) #Add predicted class to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valAUC = roc_auc_score(valData[\"signal\"].values.astype('int'), valData['pred_class'])\n",
    "print('Area under ROC curve for validation data is {:.5f}'.format(valAUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve\n",
    "In the earlier evaluations of the ROC AUC we took the data altogether giving one evaluation of the ROC and no uncertainty. Instead we can sample the predictions with replacement and evaluate the ROC on the bootstrap samples. By doing this many times we can converge to a better estimation of true ROC AUC, and get an uncertainty.\n",
    "\n",
    "This takes a while so we'll use multithreading to evaluate both development and validation performance at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aucArgs = [{'labels':valData['signal'], 'preds':valData['pred_class'], \n",
    "            'name':'Val', 'indeces':valData.index.tolist()},\n",
    "           {'labels':devData['signal'], 'preds':devData['pred_class'], \n",
    "            'name':'Dev', 'indeces':devData.index.tolist()}]\n",
    "aucs = mpRun(aucArgs, rocauc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meanScores = {}\n",
    "\n",
    "for sample in ['Dev', 'Val']:\n",
    "    meanScores[sample] = (np.mean(aucs[sample]), np.std(aucs[sample])/np.sqrt(len(aucs[sample])))\n",
    "    print(sample + ' ROC AUC, Mean = {} +- {}'.format(meanScores[sample][0], meanScores[sample][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8, 8])\n",
    "plt.plot(*roc_curve(devData['signal'].values, devData['pred_class'].values)[:2],\n",
    "         label=r'Dev, $auc={:.5f}\\pm{:.5f}$'.format(meanScores['Dev'][0], meanScores['Dev'][1]),\n",
    "         linestyle='dashed', color='b')\n",
    "plt.plot(*roc_curve(valData['signal'].values, valData['pred_class'].values)[:2],\n",
    "         label=r'Val, $auc={:.5f}\\pm{:.5f}$'.format(meanScores['Val'][0], meanScores['Val'][1]),\n",
    "         color='b')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='No discrimination')\n",
    "plt.xlabel('Background acceptance', fontsize=24, color='black')\n",
    "plt.ylabel('Signal acceptance', fontsize=24, color='black')\n",
    "plt.legend(loc='best', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MVA distribution\n",
    "We can also plot the distribution of the predicted class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {'hist' : True, 'kde' : False, 'norm_hist' : True}\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.distplot(valData[bkgVal]['pred_class'], label='Background', **params)\n",
    "sns.distplot(valData[sigVal]['pred_class'], label='Signal', **params)\n",
    "plt.legend(loc='best', fontsize=16)\n",
    "plt.xlabel(\"Class prediction\", fontsize=24, color='black')\n",
    "plt.ylabel(r\"$\\frac{1}{N}\\ \\frac{dN}{dp}$\", fontsize=24, color='black')\n",
    "plt.xlim([0,1])\n",
    "#plt.yscale('log', nonposy='clip')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples contains 9789204 events total\n"
     ]
    }
   ],
   "source": [
    "loc = \"../data/\"\n",
    "data = pandas.read_csv(loc + \"DS_1_test.csv\")\n",
    "print(\"Samples contains {} events total\".format(len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = inputPipe.transform(data[classTrainFeatures].values.astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = predict(X, ensemble, weights)\n",
    "data['Prediction'] = pandas.Series(pred[:,0], index=data.index) #Add predicted class to data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.index.name = 'Id'\n",
    "data.to_csv('../NN_set0_model0.csv', header=True, columns=['Prediction'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Train to Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {'hist' : True, 'kde' : False, 'norm_hist' : True}\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.distplot(valData['pred_class'], label='Train', **params)\n",
    "sns.distplot(data['Prediction'], label='Test', **params)\n",
    "plt.legend(loc='best', fontsize=16)\n",
    "plt.xlabel(\"Class prediction\", fontsize=24, color='black')\n",
    "plt.ylabel(r\"$\\frac{1}{N}\\ \\frac{dN}{dp}$\", fontsize=24, color='black')\n",
    "plt.xlim([0,1])\n",
    "plt.yscale('log', nonposy='clip')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save/load\n",
    "We can save the classifier and load it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights/NN_set0_model0\n"
     ]
    }
   ],
   "source": [
    "name = \"weights/NN_{}_{}\".format(varSet, classModel)\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.system(\"mkdir weights\")\n",
    "os.system(\"rm \" + name + \"*.json\")\n",
    "os.system(\"rm \" + name + \"*.h5\")\n",
    "os.system(\"rm \" + name + \"*.pkl\")\n",
    "for i, model in enumerate(ensemble): #This is the other way of saving Keras models\n",
    "    json_string = model.to_json()\n",
    "    open(\"{0}_{1}.json\".format(name, i), 'wb').write(str.encode(json_string)) #Save layout as json\n",
    "    model.save_weights(name + '_' + str(i) + '.h5') #Save weights as h5\n",
    "with open(name + '_compile.pkl', 'w') as fout: #Save compile arguments; loaded model might need recompiling\n",
    "    json.dump(compileArgs, fout)\n",
    "with open(name + '_weights.pkl', 'wb') as fout: #Save weights for ensembling\n",
    "    cPickle.dump(weights, fout)\n",
    "with open(name + '_inputPipe.pkl', 'wb') as fout: #Save the pre-processing pipeline\n",
    "    cPickle.dump(inputPipe, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ensemble = []\n",
    "weights = None\n",
    "inputPipe = None\n",
    "compileArgs = None\n",
    "with open(name + '_compile.pkl', 'r') as fin:\n",
    "    compileArgs = json.load(fin)\n",
    "for i in range(ensembleSize):\n",
    "    model = model_from_json(open(name + '_' + str(i) + '.json').read())\n",
    "    model.load_weights(name + \"_\" + str(i) + '.h5')\n",
    "    model.compile(**compileArgs)\n",
    "    ensemble.append(model)\n",
    "with open(name + '_weights.pkl', 'rb') as fin:\n",
    "    weights = cPickle.load(fin)\n",
    "with open(name + '_inputPipe.pkl', 'rb') as fin:\n",
    "    inputPipe = cPickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
