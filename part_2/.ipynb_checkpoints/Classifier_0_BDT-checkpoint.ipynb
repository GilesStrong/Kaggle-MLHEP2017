{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDT ensemble classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giles/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas\n",
    "import timeit #more accurate than time\n",
    "\n",
    "import theano\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy.stats import binned_statistic_2d\n",
    "\n",
    "from rep.estimators import XGBoostClassifier\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "import os\n",
    "import json\n",
    "\n",
    "from six.moves import cPickle #Faster than pickle\n",
    "\n",
    "import sys\n",
    "sys.path.append('../modules/')\n",
    "from MPPlot import *\n",
    "from Processors import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples contains 162209 signal events and 9649400 background events, 9811609 events total\n"
     ]
    }
   ],
   "source": [
    "loc = \"../data/\"\n",
    "dataTrain = pandas.read_csv(loc + \"DS_2_train.csv\")\n",
    "sigdataTrain = dataTrain.signal == 1.0\n",
    "bkgdataTrain = dataTrain.signal == 0.0\n",
    "print(\"Samples contains {} signal events and {} background events, {} events total\".format(\n",
    "        len(dataTrain[sigdataTrain]), len(dataTrain[bkgdataTrain]), len(dataTrain)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>event_id</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Z</th>\n",
       "      <th>TX</th>\n",
       "      <th>TY</th>\n",
       "      <th>chi2</th>\n",
       "      <th>signal</th>\n",
       "      <th>brick_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>35130</td>\n",
       "      <td>50550.007812</td>\n",
       "      <td>66924.796875</td>\n",
       "      <td>65943.0</td>\n",
       "      <td>0.639179</td>\n",
       "      <td>-0.086816</td>\n",
       "      <td>0.583319</td>\n",
       "      <td>1.0</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-999</td>\n",
       "      <td>48811.390625</td>\n",
       "      <td>65584.648438</td>\n",
       "      <td>12930.0</td>\n",
       "      <td>0.569996</td>\n",
       "      <td>0.294196</td>\n",
       "      <td>1.582101</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-999</td>\n",
       "      <td>30120.525391</td>\n",
       "      <td>28329.425781</td>\n",
       "      <td>21981.0</td>\n",
       "      <td>-0.366648</td>\n",
       "      <td>-0.379520</td>\n",
       "      <td>1.292502</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-999</td>\n",
       "      <td>67186.773438</td>\n",
       "      <td>23033.931641</td>\n",
       "      <td>68529.0</td>\n",
       "      <td>-0.434561</td>\n",
       "      <td>0.502130</td>\n",
       "      <td>2.832125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-999</td>\n",
       "      <td>64832.343750</td>\n",
       "      <td>64279.328125</td>\n",
       "      <td>7758.0</td>\n",
       "      <td>0.139993</td>\n",
       "      <td>-0.417299</td>\n",
       "      <td>2.959314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  event_id             X             Y        Z        TX        TY  \\\n",
       "0      0     35130  50550.007812  66924.796875  65943.0  0.639179 -0.086816   \n",
       "1      1      -999  48811.390625  65584.648438  12930.0  0.569996  0.294196   \n",
       "2      2      -999  30120.525391  28329.425781  21981.0 -0.366648 -0.379520   \n",
       "3      3      -999  67186.773438  23033.931641  68529.0 -0.434561  0.502130   \n",
       "4      4      -999  64832.343750  64279.328125   7758.0  0.139993 -0.417299   \n",
       "\n",
       "       chi2  signal  brick_number  \n",
       "0  0.583319     1.0            53  \n",
       "1  1.582101     0.0            49  \n",
       "2  1.292502     0.0            39  \n",
       "3  2.832125     0.0            65  \n",
       "4  2.959314     0.0            13  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataTrain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create development and validation samples\n",
    "Development dataTrain is used for training and testing. Validation is used for testing the final classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This defines lists of indices for signal and background events for the development and validation samples. About 20% of each class is reserved for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are more background events, let's define weights to help balance out the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Weights are:  {0.0: 1.0, 1.0: 59.487451374461344}\n"
     ]
    }
   ],
   "source": [
    "classWeights = {0.0 : 1.0,\n",
    "                1.0 : len(dataTrain[bkgdataTrain])/len(dataTrain[sigdataTrain])}\n",
    "print(\"Dev Weights are: \", classWeights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XY weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "signalXY = binned_statistic_2d(devdataTrain.ix[sigDev, 'X'].values,\n",
    "                                  devdataTrain.ix[sigDev, 'Y'].values,\n",
    "                                  devdataTrain.ix[sigDev, 'Y'].values,\n",
    "                                  statistic='count', expand_binnumbers=True,\n",
    "                                  range=((np.min(devdataTrain.ix[sigDev, 'X'].values),\n",
    "                                          np.max(devdataTrain.ix[sigDev, 'X'].values)),\n",
    "                                         (np.min(devdataTrain.ix[sigDev, 'Y'].values),\n",
    "                                          np.max(devdataTrain.ix[sigDev, 'Y'].values))),\n",
    "                                  bins=(50, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "devdataTrain.ix[sigDev, 'weight'] = 1/signalXY.statistic[signalXY.binnumber[0]-1, signalXY.binnumber[1]-1]\n",
    "devdataTrain.ix[sigDev, 'weight'] = devdataTrain.ix[sigDev, 'weight']/np.sum(devdataTrain.ix[sigDev, 'weight'])\n",
    "devdataTrain.ix[bkgDev, 'weight'] = 1/len(devdataTrain[bkgDev])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Options\n",
    "Define the features used for discrimination and training options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainFeatures = ['X', 'Y', 'Z', 'TX', 'TY', 'chi2', 'brick_number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set0 = ['X', 'Y', 'Z', 'TX', 'TY', 'chi2', 'brick_number']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training options\n",
    "Here we define the way we'll train the classifier. For simplicity we'll just use the low-level final-state features. We can also choose what pre-processing step to apply to the dataTrain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 6 features ['X', 'Y', 'Z', 'TX', 'TY', 'chi2']\n"
     ]
    }
   ],
   "source": [
    "classTrainFeatures = set0 #The features used\n",
    "classModel = \"vxgb\" #Will define the layout of the network\n",
    "varSet = \"set0\" #Name of the feature set used, mainly for saving results\n",
    "normIn = True #Whether we want to normalise and standardise the inputs\n",
    "pca = True #Whether we want to use principal-component analysis to decorrelate inputs\n",
    "whiten = False #Whether we want to whiten input dataTrain\n",
    "normPCA = True #Whether we want to normalise and standardise the inputs after PCA\n",
    "nSplits = 10 #Number of train/test splits to make during cross-validation\n",
    "useClassWeights = False\n",
    "useSampleWeights = False\n",
    "ensembleSize = 10 #Number of classifiers  to include in ensemble = min(nSplits, ensembleSize)\n",
    "nSplit = 10\n",
    "ensembleMode = 'AUC' #Metric used to weight classifiers in ensemble, I've found loss to quite relaible\n",
    "print(\"Training on {} features {}\". format(len(classTrainFeatures),[var for var in classTrainFeatures]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process dataTrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a SK-Learn pipeline which will contain transformation steps for any dataTrain fed in. Pipelines are a nice, compact way of handing dataTrain transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stepsIn = []\n",
    "if not normIn and not pca:\n",
    "    stepsIn.append(('ident', StandardScaler(with_mean=False, with_std=False))) #For compatability\n",
    "else:\n",
    "    if normIn:\n",
    "        stepsIn.append(('normIn', StandardScaler()))\n",
    "    if pca:\n",
    "        stepsIn.append(('pca', PCA(whiten=whiten)))\n",
    "        if normPCA:\n",
    "            stepsIn.append(('normPCA', StandardScaler()))\n",
    "inputPipe = Pipeline(stepsIn)\n",
    "stepsOut = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we fit the pipeline to the **development** dataTrain inputs. For compactness we also transform the development dataTrain and create Numpy arrays of the inputs and targets. **N.B.** The type of the inputs will normally be either float32 or float64. float32 is preferred, since speed and memory outweighs precision. Sometimes if the dataTrain is naturally in float64, the conversion to float32 can can result in NaNs or infs, so watch out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputPipe.fit(dataTrain[classTrainFeatures].values.astype('float32')) #Fit to signal\n",
    "X_class = inputPipe.transform(dataTrain[classTrainFeatures].values.astype('float32'))\n",
    "y_class = dataTrain['signal'].values.astype('int') #Outputs\n",
    "if useSampleWeights: X_weights = dataTrain['weight'].values.astype('float64') #Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define classifier\n",
    "Here we define the layout of the neural network. The basic class is sequential. The network is them defined by adding dense layers (which contain the neurons and weights) and then the activation function. The activation function can be defined directly in the dense constructor, however adding separately gives a clearer picture of the network, and allows it to be easily replaced with an advanced activation-function.\n",
    "\n",
    "For contempory ML, the rectified linear unit is generally the default activation function. Since we want the outputs to be between 0 (background) and 1 (signal) we'll use the sigmoid function as the final activation function, since it saturates at these values and is symmetric between them.\n",
    "\n",
    "The compile step combines the choice of loss function and optimiser into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getClassifier(model):\n",
    "    classModel = None\n",
    "    if model == 'vxgb':\n",
    "        classModel = XGBoostClassifier()\n",
    "    return classModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to train the classifier.\n",
    "\n",
    "We use stratified k-fold cross-validation for training. This splits the full development dataTrain into *k* sets, trains the model on $k-1$ sets and tests its performance on the remainings set. This then continues *k* times with a different set being used for testing each time. The *stratified* part means that each set will contain the same fraction of event classes as the full dataTrainset, which helps ensure unbiased training and means that our class weights will be valid.\n",
    "\n",
    "During training we save each trained model, as well as its performance on the test set.\n",
    "\n",
    "**N.B.** The model can either be saved directly, or by saving the weights and layout separately. The former is more compact, but doesn't handle custom objects well. If you've used a custom loss or activation function, the the second method is more flexible. It also seems to be quicker, but I've not done concrete tests..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 1 / 10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-34012587cb7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_class\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_class\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mweightParams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_class\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_class\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mweightParams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'AUC'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_class\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_class\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/giles/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/rep-0.6.5-py2.7.egg/rep/estimators/xgboost.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_classes_\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'multi:softprob'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_classes_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/giles/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/rep-0.6.5-py2.7.egg/rep/estimators/xgboost.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, estimator_type, sample_weight, **kwargs)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m             \u001b[0mxgmat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_dmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxgboost_classifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxgmat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_boost_round\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'There is error in the parameters or in input data format.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/giles/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/xgboost/training.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, learning_rates, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m    203\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/giles/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/xgboost/training.pyc\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[0mversion\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/giles/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/xgboost/core.pyc\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m    804\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    805\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 806\u001b[1;33m             \u001b[0m_check_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBoosterUpdateOneIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    807\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    808\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "weightParams = {}\n",
    "\n",
    "if useClassWeights:\n",
    "    print(\"Using class weights\")\n",
    "    weightParams['class_weight'] = classWeights\n",
    "\n",
    "if useSampleWeights:\n",
    "    print(\"Using sample weights\")\n",
    "    \n",
    "results = []\n",
    "histories = []\n",
    "os.system(\"rm train_weights/*.pkl\")\n",
    "skf = StratifiedKFold(n_splits=nSplits, shuffle=True)\n",
    "i = 0\n",
    "for train, test in skf.split(X_class, y_class):\n",
    "    i += 1\n",
    "    print(\"Running fold\", i, \"/\", nSplit)\n",
    "    model = None # Clearing the classifier\n",
    "    model = getClassifier(classModel)\n",
    "    if useSampleWeights:\n",
    "        model.fit(X_class[train], y_class[train], sample_weight=X_weights[train], **weightParams)\n",
    "    else:\n",
    "        model.fit(X_class[train], y_class[train], **weightParams)\n",
    "    results.append({})\n",
    "    results[-1]['AUC'] = 1-roc_auc_score(y_class[test], model.predict_proba(X_class[test])[:,1])\n",
    "    with open('train_weights/train_' + str(i-1) + '.pkl', 'w') as fout:\n",
    "        pickle.dump(model, fout)\n",
    "    print(\"Score is:\", results[-1])\n",
    "with open('train_weights/resultsFile.pkl', 'w') as fout:\n",
    "    pickle.dump(results, fout)\n",
    "print(\"Cross-validation took {:.3f}s \".format(timeit.default_timer() - start))\n",
    "X_class = None\n",
    "y_class = None\n",
    "train = None\n",
    "test = None\n",
    "model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot histories\n",
    "Now let's plot the history of the training.\n",
    "\n",
    "We can see that the test loss starts to decrease, reaches a minimum point, then begins to increase. The training loss continues to decrease. The early stopping detects the lack of imporvement in test loss and stops the training. The checkpoint allows us to use the state of the model at the minimum point of training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "for i, history in enumerate(histories):\n",
    "    if i == 0:\n",
    "        plt.plot(history['loss'], color='g', label='Training')\n",
    "        plt.plot(history['val_loss'], color='b', label='Testing')\n",
    "    else:\n",
    "        plt.plot(history['loss'], color='g')\n",
    "        plt.plot(history['val_loss'], color='b')\n",
    "        \n",
    "plt.legend(fontsize=16)\n",
    "plt.xlabel(\"Epoch\", fontsize=24, color='black')\n",
    "plt.ylabel(\"Loss\", fontsize=24, color='black')\n",
    "#plt.ylim((0.0, 0.2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct ensemble\n",
    "During the *k*-fold CV we trained *k* models. We could just use the best one, however it is unlikely to optimimum for all input possibilities. Ensembling is a method of using multiple classifiers together to achieve a better result than a single one on its own.\n",
    "\n",
    "The method I use here is to weight the contributions of each classifier according to how well it performed on its test set during CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = None\n",
    "with open('train_weights/resultsFile.pkl', 'rb') as fin: #Reload results in case notebook was closed\n",
    "    results = cPickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadModel(cycle, location='train_weights/train_'): #Function to load a specified classifier\n",
    "    cycle = int(cycle)\n",
    "    model = load_model(location + str(cycle) + '.h5')\n",
    "    model.compile(**compileArgs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getWeights(value, met): #How the weight is calculated. Metrics configured such that lower values are better.\n",
    "    return 1/value #Reciprocal of metric is a simple way of assigning larger weight s to better metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we order the classifiers by performance, load the required number, and weight them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ensemble = []\n",
    "weights = []\n",
    "\n",
    "print(\"Choosing ensemble by\", ensembleMode)\n",
    "dtype = [('cycle', int), ('result', float)]\n",
    "values = np.sort(np.array([(i, result[ensembleMode]) for i, result in enumerate(results)], \n",
    "                          dtype=dtype), order=['result'])\n",
    "\n",
    "for i in range(min([ensembleSize, len(results)])):\n",
    "    ensemble.append(loadModel(values[i]['cycle']))\n",
    "    weights.append(getWeights(values[i]['result'], ensembleMode))\n",
    "    print(\"Model {} is {} with {} = {}\". format(i, values[i]['cycle'], ensembleMode, values[i]['result']))\n",
    "    \n",
    "weights = np.array(weights)\n",
    "weights = weights/weights.sum() #normalise weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response on dev dataTrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply the ensemble to the whole of the development dataTrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-process dataTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dev = inputPipe.transform(devdataTrain[classTrainFeatures].values.astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict(indataTrain, ensemble, weights, n=-1): #Loop though each classifier and predict dataTrain class\n",
    "    pred = np.zeros((len(indataTrain), 1))\n",
    "    if n == -1:\n",
    "        n = len(ensemble)+1\n",
    "    ensemble = ensemble[0:n] #Use only specified number of classifiers\n",
    "    weights = weights[0:n]\n",
    "    weights = weights/weights.sum() #Renormalise weights\n",
    "    \n",
    "    for i, model in enumerate(ensemble):\n",
    "        pred += weights[i] * model.predict(indataTrain, verbose=0)\n",
    "        \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = predict(X_dev, ensemble, weights)\n",
    "devdataTrain['pred_class'] = pandas.Series(pred[:,0], index=devdataTrain.index) #Add predicted class to dataTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "devAUC = roc_auc_score(devdataTrain[\"signal\"].values.astype('int'), devdataTrain['pred_class'])\n",
    "print('Area under ROC curve for development dataTrain is {:.5f}'.format(devAUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble testing\n",
    "What benefit does ensembling bring? Let's take a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(ensembleSize):\n",
    "    auc = roc_auc_score(devdataTrain[\"signal\"].values.astype('int'),\n",
    "                        predict(X_dev, ensemble, weights, i+1))\n",
    "    if not i:\n",
    "        print(\"AUC using best classifier:\\t{:.5f}\".format(auc))\n",
    "    else:\n",
    "        print(\"AUC using {} classifiers:\\t{:.5f}\".format(i+1, auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definite improvements seen, but the improvements will eventually saturate. The number to use and the ways the weights are calculated are hyperparameters of the MVA. We could have set aside some of the development dataTrain for tuning this, but for now we'll just stick with what we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response on val dataTrain\n",
    "Having done all the development of the classifier, we're now ready to do final testing on the withheld validation dataTrain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-process dataTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_val = inputPipe.transform(valdataTrain[classTrainFeatures].values.astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = predict(X_val, ensemble, weights)\n",
    "valdataTrain['pred_class'] = pandas.Series(pred[:,0], index=valdataTrain.index) #Add predicted class to dataTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valAUC = roc_auc_score(valdataTrain[\"signal\"].values.astype('int'), valdataTrain['pred_class'])\n",
    "print('Area under ROC curve for validation dataTrain is {:.5f}'.format(valAUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble testing\n",
    "Again we can confirm that ensembling helps. **N.B.** Do not use the validation dataTrain for any tuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(ensembleSize):\n",
    "    auc = roc_auc_score(valdataTrain[\"signal\"].values.astype('int'),\n",
    "                              predict(X_val, ensemble, weights, i+1))\n",
    "    if not i:\n",
    "        print(\"AUC using best classifier:\\t{:.5f}\".format(auc))\n",
    "    else:\n",
    "        print(\"AUC using {} classifiers:\\t{:.5f}\".format(i+1, auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct ensemble\n",
    "During the *k*-fold CV we trained *k* models. We could just use the best one, however it is unlikely to optimimum for all input possibilities. Ensembling is a method of using multiple classifiers together to achieve a better result than a single one on its own.\n",
    "\n",
    "The method I use here is to weight the contributions of each classifier according to how well it performed on its test set during CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensembleSize = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = None\n",
    "with open('train_weights/resultsFile.pkl', 'rb') as fin: #Reload results in case notebook was closed\n",
    "    results = cPickle.load(fin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we order the classifiers by performance, load the required number, and weight them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ensemble = []\n",
    "weights = []\n",
    "\n",
    "print(\"Choosing ensemble by\", ensembleMode)\n",
    "dtype = [('cycle', int), ('result', float)]\n",
    "values = np.sort(np.array([(i, result[ensembleMode]) for i, result in enumerate(results)], \n",
    "                          dtype=dtype), order=['result'])\n",
    "\n",
    "for i in range(min([ensembleSize, len(results)])):\n",
    "    ensemble.append(loadModel(values[i]['cycle']))\n",
    "    weights.append(getWeights(values[i]['result'], ensembleMode))\n",
    "    print(\"Model {} is {} with {} = {}\". format(i, values[i]['cycle'], ensembleMode, values[i]['result']))\n",
    "    \n",
    "weights = np.array(weights)\n",
    "weights = weights/weights.sum() #normalise weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response on dev dataTrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply the ensemble to the whole of the development dataTrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process dataTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dev = inputPipe.transform(devdataTrain[classTrainFeatures].values.astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = predict(X_dev, ensemble, weights)\n",
    "devdataTrain['pred_class'] = pandas.Series(pred[:,0], index=devdataTrain.index) #Add predicted class to dataTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "devAUC = roc_auc_score(devdataTrain[\"signal\"].values.astype('int'), devdataTrain['pred_class'])\n",
    "print('Area under ROC curve for development dataTrain is {:.5f}'.format(devAUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response on val dataTrain\n",
    "Having done all the development of the classifier, we're now ready to do final testing on the withheld validation dataTrain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process dataTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_val = inputPipe.transform(valdataTrain[classTrainFeatures].values.astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = predict(X_val, ensemble, weights)\n",
    "valdataTrain['pred_class'] = pandas.Series(pred[:,0], index=valdataTrain.index) #Add predicted class to dataTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valAUC = roc_auc_score(valdataTrain[\"signal\"].values.astype('int'), valdataTrain['pred_class'])\n",
    "print('Area under ROC curve for validation dataTrain is {:.5f}'.format(valAUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve\n",
    "In the earlier evaluations of the ROC AUC we took the dataTrain altogether giving one evaluation of the ROC and no uncertainty. Instead we can sample the predictions with replacement and evaluate the ROC on the bootstrap samples. By doing this many times we can converge to a better estimation of true ROC AUC, and get an uncertainty.\n",
    "\n",
    "This takes a while so we'll use multithreading to evaluate both development and validation performance at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aucArgs = [{'labels':valdataTrain['signal'], 'preds':valdataTrain['pred_class'], \n",
    "            'name':'Val', 'indeces':valdataTrain.index.tolist()},\n",
    "           {'labels':devdataTrain['signal'], 'preds':devdataTrain['pred_class'], \n",
    "            'name':'Dev', 'indeces':devdataTrain.index.tolist()}]\n",
    "aucs = mpRun(aucArgs, rocauc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meanScores = {}\n",
    "\n",
    "for sample in ['Dev', 'Val']:\n",
    "    meanScores[sample] = (np.mean(aucs[sample]), np.std(aucs[sample])/np.sqrt(len(aucs[sample])))\n",
    "    print(sample + ' ROC AUC, Mean = {} +- {}'.format(meanScores[sample][0], meanScores[sample][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8, 8])\n",
    "plt.plot(*roc_curve(devdataTrain['signal'].values, devdataTrain['pred_class'].values)[:2],\n",
    "         label=r'Dev, $auc={:.5f}\\pm{:.5f}$'.format(meanScores['Dev'][0], meanScores['Dev'][1]),\n",
    "         linestyle='dashed', color='b')\n",
    "plt.plot(*roc_curve(valdataTrain['signal'].values, valdataTrain['pred_class'].values)[:2],\n",
    "         label=r'Val, $auc={:.5f}\\pm{:.5f}$'.format(meanScores['Val'][0], meanScores['Val'][1]),\n",
    "         color='b')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='No discrimination')\n",
    "plt.xlabel('Background acceptance', fontsize=24, color='black')\n",
    "plt.ylabel('Signal acceptance', fontsize=24, color='black')\n",
    "plt.legend(loc='best', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MVA distribution\n",
    "We can also plot the distribution of the predicted class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {'hist' : True, 'kde' : False, 'norm_hist' : True}\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.distplot(valdataTrain[bkgVal]['pred_class'], label='Background', **params)\n",
    "sns.distplot(valdataTrain[sigVal]['pred_class'], label='Signal', **params)\n",
    "plt.legend(loc='best', fontsize=16)\n",
    "plt.xlabel(\"Class prediction\", fontsize=24, color='black')\n",
    "plt.ylabel(r\"$\\frac{1}{N}\\ \\frac{dN}{dp}$\", fontsize=24, color='black')\n",
    "plt.xlim([0,1])\n",
    "#plt.yscale('log', nonposy='clip')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loc = \"../data/\"\n",
    "dataTest = pandas.read_csv(loc + \"DS_1_test.csv\")\n",
    "print(\"Samples contains {} events total\".format(len(dataTest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process dataTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataTest['reg_P'] = regOutputPipe.inverse_transform(regressor.predict(\n",
    "        regInputPipe.transform(dataTest[regTrainFeatures].values.astype('float32')), verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = inputPipe.transform(dataTest[classTrainFeatures].values.astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = predict(X, ensemble, weights)\n",
    "dataTest['Prediction'] = pandas.Series(pred[:,0], index=dataTest.index) #Add predicted class to dataTrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataTest.index.name = 'Id'\n",
    "dataTest.to_csv('NN_set1_model5.csv', header=True, columns=['Prediction'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Train to Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {'hist' : True, 'kde' : False, 'norm_hist' : True}\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.distplot(valdataTrain['pred_class'], label='Train', **params)\n",
    "sns.distplot(dataTest['Prediction'], label='Test', **params)\n",
    "plt.legend(loc='best', fontsize=16)\n",
    "plt.xlabel(\"Class prediction\", fontsize=24, color='black')\n",
    "plt.ylabel(r\"$\\frac{1}{N}\\ \\frac{dN}{dp}$\", fontsize=24, color='black')\n",
    "plt.xlim([0,1])\n",
    "plt.yscale('log', nonposy='clip')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save/load\n",
    "We can save the classifier and load it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name = \"weights/NN_PreWeight_ClassWeight_{}_{}\".format(varSet, classModel)\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.system(\"mkdir weights\")\n",
    "os.system(\"rm \" + name + \"*.json\")\n",
    "os.system(\"rm \" + name + \"*.h5\")\n",
    "os.system(\"rm \" + name + \"*.pkl\")\n",
    "for i, model in enumerate(ensemble): #This is the other way of saving Keras models\n",
    "    json_string = model.to_json()\n",
    "    open(\"{0}_{1}.json\".format(name, i), 'wb').write(str.encode(json_string)) #Save layout as json\n",
    "    model.save_weights(name + '_' + str(i) + '.h5') #Save weights as h5\n",
    "with open(name + '_compile.pkl', 'w') as fout: #Save compile arguments; loaded model might need recompiling\n",
    "    json.dump(compileArgs, fout)\n",
    "with open(name + '_weights.pkl', 'wb') as fout: #Save weights for ensembling\n",
    "    cPickle.dump(weights, fout)\n",
    "with open(name + '_inputPipe.pkl', 'wb') as fout: #Save the pre-processing pipeline\n",
    "    cPickle.dump(inputPipe, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ensemble = []\n",
    "weights = None\n",
    "inputPipe = None\n",
    "compileArgs = None\n",
    "with open(name + '_compile.pkl', 'r') as fin:\n",
    "    compileArgs = json.load(fin)\n",
    "for i in range(ensembleSize):\n",
    "    model = model_from_json(open(name + '_' + str(i) + '.json').read())\n",
    "    model.load_weights(name + \"_\" + str(i) + '.h5')\n",
    "    model.compile(**compileArgs)\n",
    "    ensemble.append(model)\n",
    "with open(name + '_weights.pkl', 'rb') as fin:\n",
    "    weights = cPickle.load(fin)\n",
    "with open(name + '_inputPipe.pkl', 'rb') as fin:\n",
    "    inputPipe = cPickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
